---
layout: post
title: "[Hadoop Solution] Simple Prediction System for a automobile company"
date: '2014-11-15T08:24:00.002-08:00'
author: Akash Mishra
tags:
- Java
- bigdata
- hadoop
- hive
- oozie
- Map reduce
modified_time: '2014-11-15T09:15:02.222-08:00'
blogger_id: tag:blogger.com,1999:blog-5454104814334713963.post-6461470604873051424
blogger_orig_url: http://sleepythread.blogspot.com/2014/11/hadoop-solution-simple-predection.html
---

<div dir="ltr" style="text-align: left;" trbidi="on"><div dir="ltr" style="text-align: left;" trbidi="on"><br /><br />In my previous post, I mentioned a sample <a href="http://sleepythread.blogspot.in/2014/10/hadoop-problem-simple-predection-system.html">Hadoop problem</a>. I have created a sample solution for the mentioned problem and uploaded it to <a href="https://github.com/SleepyThread/prediction-system">github</a>.<br /><br />This post explain on how to run the solution on the <a href="http://sleepythread.blogspot.in/2014/11/setting-up-hadoop-cluster-using.html">sample cluster</a> and component of solutions.<br /><br /><h3 style="text-align: left;">Set Up Data:</h3>As per our problem statement, we need some data regarding various vehicle and their usage rate. There is a seed folder which contains the seed file and Schema' for vehicle and usage data.<br /><br /><ul style="text-align: left;"><li>create_table.hql file contains all the hive schema information for given problem.</li><li>usage_data.csv file contains comma separated vehicle usage information.</li><li>vehicle_data.csv file contains comma separated vehicle information.</li></ul>If you want to setup the seed data into your cluster, then you just need to update <a href="https://github.com/SleepyThread/prediction-system/blob/master/seed/prediction_system_setup.sh">prediction_setup_script.sh</a> with hive location and run the shell script. It will create schema and upload the data to respective location in the cluster. <br /><h3 style="text-align: left;">Orchestrating ETL flow:</h3>Most of the big data application, data is processed in lots of stages. Oozie is one of the popular Orchestration system for Hadoop applications. Oozie uses xml to specify the whole pipeline of ETL.<br />You can view the whole workflow xml in solution <a href="https://github.com/SleepyThread/prediction-system/blob/master/oozie/workflow/workflow.xml">here</a>.&nbsp; <br /><br />In our solution's there are 2 stages which data will be processed,<br /><br />Stage 1: Associating the Vehicle data with usage information. <br />Stage 2: Generating events with the help of data generated in Step 1.<br /><br /><br /><h4 style="text-align: left;">Stage 1:</h4>We have two hive tables in the system, "vehicle" &amp; "usage".&nbsp; We can use hive to JOIN two table and create a de-normalize table which will contain both vehicle and usage. <br /><pre class="brush: xml">  &lt;action name="JoinVehicleAndUsageRate"&gt;<br />        &lt;hive xmlns="uri:oozie:hive-action:0.2"&gt;<br />            &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;<br />            &lt;name-node&gt;${nameNode}&lt;/name-node&gt;<br />            &lt;job-xml&gt;hive-site-conf.xml&lt;/job-xml&gt;<br />            &lt;configuration&gt;<br />                &lt;property&gt;<br />                    &lt;name&gt;mapred.job.queue.name&lt;/name&gt;<br />                    &lt;value&gt;${queueName}&lt;/value&gt;<br />                &lt;/property&gt;<br />                &lt;property&gt;<br />                    &lt;name&gt;oozie.hive.defaults&lt;/name&gt;<br />                    &lt;value&gt;hive-site-conf.xml&lt;/value&gt;<br />                &lt;/property&gt;<br />            &lt;/configuration&gt;<br />            &lt;script&gt;join-vehicle-and-usage.hql&lt;/script&gt;<br />            &lt;param /&gt;location=${inputDir}<br />            &lt;file&gt;hive-site-conf.xml#hive-site-conf.xml&lt;/file&gt;<br />        &lt;/hive&gt;<br />        &lt;ok to="GenerateServicingEvent"&gt;<br />        &lt;error to="kill"&gt;<br />    &lt;/error&gt;&lt;/ok&gt;&lt;/action&gt;<br />   <br /></pre><br />This XML node will execute the JOIN query inside <a href="https://github.com/SleepyThread/prediction-system/blob/master/oozie/workflow/join-vehicle-and-usage.hql">join-vehicle-and-usage.hql</a> file.<br /><h4 style="text-align: left;">Stage 2:</h4>Once we have De-normalized data and each vehicle is associated with its usage information, we can now calculate all the events. We will perform this step as a Map-Reduce program. In workflow.xml this step is specified as<br /><br /><pre class="brush: xml">    &lt;action name="GenerateServicingEvent"&gt;<br />          &lt;java&gt;<br />            &lt;job-tracker&gt;${jobTracker}&lt;/job-tracker&gt;<br />            &lt;name-node&gt;${nameNode}&lt;/name-node&gt;<br />            &lt;prepare&gt;<br />              &lt;delete path="${outputDir}"/&gt;<br />            &lt;/prepare&gt;<br />            &lt;configuration&gt;<br />              &lt;property&gt;<br />                &lt;name&gt;mapred.job.queue.name&lt;/name&gt;<br />                &lt;value&gt;${queueName}&lt;/value&gt;<br />              &lt;/property&gt;<br />            &lt;/configuration&gt;<br />            &lt;main-class&gt;com.sleepythread.EventGeneratorDriver&lt;/main-class&gt;<br />            &lt;java-opts&gt;-DinputPath=${inputDir} -DoutputPath=${outputDir}&lt;/java-opts&gt;<br />            &lt;arg&gt;-libjars&lt;/arg&gt;<br />            &lt;arg&gt;${nameNode}/user/hadoop/workflow/lib/joda-time-2.3.jar&lt;/arg&gt;<br />          &lt;/java&gt;<br />        &lt;ok to="end"/&gt;<br />        &lt;error to="kill"/&gt;<br />    &lt;/action&gt;<br /> <br /></pre><br /><br />This step will execute the Map/Reduce program using <a href="https://github.com/SleepyThread/prediction-system/blob/master/src/main/java/com/sleepythread/EventGeneratorDriver.java">EventGeneratorDriver</a> program in the code. <br /><br /><br /><i><u><b>Event Generation:</b></u></i><br /><br />Event generation is done by <b>map only</b> map-reduce program. Map phase is specified in <a href="https://github.com/SleepyThread/prediction-system/blob/master/src/main/java/com/sleepythread/EventGeneratorMapper.java">EventGeneratorMapper.java</a> file.<br /><br />In Map phase, first we get the max date till which we have to generate servicing events. This information is passed by driver using context object.<br /><br /><pre class="brush: java">Configuration configuration = context.getConfiguration();<br />DateTimeFormatter dateTimeFormatter = DateTimeFormat.forPattern("yyyy-MM-dd");<br />DateTime endDate = DateTime.parse(configuration.get("endDate"),dateTimeFormatter);<br /></pre><br /><br />Once we have end date then we parse the each record and get corresponding information from the row. Hadoop calls map function for each record present in the HDFS.<br /><br /><pre class="brush: java">        String[] split = record.toString().split(HIVE_COLUMN_SEPARATOR);<br /><br />        Double id = Double.parseDouble(split[0]);<br />        Double interval = Double.parseDouble(split[1]);<br />        Double rate = Double.parseDouble(split[2]);<br />        Double current_usage = Double.parseDouble(split[3]);<br />        DateTime capture_date = DateTime.parse(split[4],dateTimeFormatter);<br /><br /></pre><br />Once we have all information, we calculate the first event after the capture date and number of days after each event occurs [Event are linear in nature and happens at regular interval's].<br /><br /><pre class="brush: java">   Double nextServicingKm = Math.ceil(current_usage / interval)*rate;<br />   Double noOfDaysToReachNextServingKm = (nextServicingKm - current_usage)/rate;<br />   DateTime nextServicingDate = capture_date.plusDays(noOfDaysToReachNextServingKm.intValue());<br />   Double daysToCompleteIntervalUsingRate = (interval/rate);<br /></pre><br /><br />We can now just perform linear addition till we hit the max-date [end-date]. We calculate each event occurrence and write that to HDFS output file.<br /><br /><pre class="brush: java">   while (rate != 0 &amp;&amp; nextServicingDate.isBefore(endDate)){<br />      Text text = new Text(id.intValue()+HIVE_COLUMN_SEPARATOR<br />                      +nextServicingKm.intValue()+<br />                      HIVE_COLUMN_SEPARATOR+<br />                      dateTimeFormatter.print(nextServicingDate));<br />      context.write(NullWritable.get(), text);<br /><br />      nextServicingKm = nextServicingKm + interval;<br />      nextServicingDate = nextServicingDate.plusDays(<br />                          daysToCompleteIntervalUsingRate.intValue());<br />   }<br /></pre><br /><br /><u><i><b>Unit Testing:</b></i></u><br /><br />No Application is good until you have unit test in place. I have also added <a href="https://github.com/SleepyThread/prediction-system/blob/master/src/test/com/sleepythread/EventGeneratorMapperTest.java">Test file</a> in the codebase which test some case of event generation process. <br /><br /><h3 style="text-align: left;">Executing Above Application on Cluster:</h3>To run the application on the cluster [You can install a cluster on cloud, refer this blog.]. Perform following step:<br /><h4 style="text-align: left;">Step 1.&nbsp; Clone solution from github.</h4><div style="text-align: left;"><pre class="brush: bash">git clone git@github.com:SleepyThread/prediction-system.git</pre></div></div><div style="text-align: left;"><br /></div><div style="text-align: left;"><h4 style="text-align: left;">Step 2: Seed the data on the cluster.</h4></div><div style="text-align: left;"><pre class="brush: bash">## Copy the seed file <br /><br />scp prediction-system/  hadoop@<node>:/home/hadoop<br /><br />## SSH onto machine in with hive client is install<br /><br />ssh hadoop@<node></node></node><node><node>&lt;remote-node&gt;<br /><br />## Change the Hive location in prediction_system_setup.sh<br />## Give execute permission to the shell script<br /><br />chmod u+x seed/prediction_system_setup.sh<br /><br />## Run the shell file. <br /><br />./seed/prediction_system_setup.sh<br /><br /></node></node></pre></div><h4 style="text-align: left;">Step 3:&nbsp; Update various parameter with respect to our cluster.</h4><br />Now we need to update some parameter in order to execute the application on the cluster.<br /><br />1. Copy &amp; paste <a href="https://github.com/SleepyThread/prediction-system/blob/master/oozie/resource/job_sample.properties">job_sample.properties</a> to job.properties in same dir. <br />2. Update <span class="pl-k">nameNode</span>= &amp; <span class="pl-k">jobTracker</span>= in job.properties with namenode &amp; resource manager of your cluster.<br />3. Update the <a href="https://github.com/SleepyThread/prediction-system/blob/master/oozie/workflow/hive-site-conf.xml">hive configuration file</a> with thrift server address of your cluster. <br />4. Update the Oozie Server information in <a href="https://github.com/SleepyThread/prediction-system/blob/master/oozie/bin/run_job.sh">run_job.sh</a> file. <br />&nbsp; <br /><br /><h4 style="text-align: left;">Step 4: Build the package using maven &amp; transfer generated tar file to server on which oozie client is installed. </h4><br /><pre class="brush: bash"># run maven package command<br /><br />mvn clean package <br /><br /># SCP the generated tar file to remote machine<br />&nbsp;<br />scp target/prediction-system-0.1-workflow-package.tar.gz hadoop@&lt;node&gt;:/home/hadoop<br /><br /></pre><h4 style="text-align: left;">Step 5: SSH into the remote machine and run the Application  </h4><pre class="brush: bash">## transfer tar.gz to edgenode.<br /><br />&nbsp;scp target/prediction-system-0.1-workflow-package.tar.gz&nbsp;</pre><pre class="brush: bash">     hadoop@&lt;remote-node&gt;:/home/hadoop<br /><br /><br />## SSH into the machine<br /><br />ssh hadoop@&lt;remote-node&gt;<br /><br />## Un-Tar the application<br /><br />tar -xvf&nbsp; prediction-system-0.1-workflow-package.tar.gz<br /><br />## Run the Application<br /><br />sh ./prediction-system-0.1/bin/run_job.sh<br /><br /></pre><br /><br />Your application will start running and you can view on your Oozie Server or Hue Server. <br /><br /><br /><br /></div>