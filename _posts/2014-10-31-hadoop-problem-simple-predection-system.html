---
layout: post
title: "[Hadoop Problem] Simple Prediction System for a automobile company"
date: '2014-10-31T11:09:00.000-07:00'
author: Akash Mishra
tags:
- bigdata
- hadoop
modified_time: '2014-11-15T08:48:20.682-08:00'
blogger_id: tag:blogger.com,1999:blog-5454104814334713963.post-4720190859260487795
blogger_orig_url: http://sleepythread.blogspot.com/2014/10/hadoop-problem-simple-predection-system.html
---

<div dir="ltr" style="text-align: left;" trbidi="on"><br /><br />This is a sample application by which i will try to explain usage of various different Big Data system like Hadoop.<br /><br /><h3 style="text-align: left;">Problem Statement:</h3>Lets say some automobile company predict all the servicing details of all the vehicles, which they have sold.<br /><br /><h4 style="text-align: left;">Data Source:</h4>Vehicles Details:<br /><ul style="text-align: left;"><li>&nbsp;Id</li><li>Name</li><li>State</li><li>Servicing Interval (km)</li></ul>Usage Details:<br /><ul style="text-align: left;"><li>Vehicle id</li><li>Usage Rate (per day)</li><li>Current Usage (K.m)</li><li>Capture Date </li></ul><h4 style="text-align: left;">Prediction Algorithm:</h4>This system will use a very simple arithmetic progression technique to determine servicing opportunity.<br /><br />e.g:<br /><br />Lets say following details are present for a vehicle,<br /><br />Id: 1234<br />Name: I10<br />State: Uttar Pradesh<br />Interval: 10000<br /><br />Usage Details are:<br />Usage Rate: 100<br />Current Usage: 20000<br />Capture Date: 01-01-2014<br /><br />By Using current Usage Rate (100) and Servicing Interval (10000), we can predict that service will happen at<br /><br />Interval/Rate = 100 days<br /><br />So The Servicing Event for Vehicle ID: 1234 will happen at<br /><br />11-April-2014<br />20-July-2014....<br /><br /><h4 style="text-align: left;">&nbsp;How Big Data?</h4>Well the above problem is very simple and first thing comes in our mind is that this problem statement could be solved using traditional system. So why need Hadoop to solve this problem.<br /><br />Answer to this question's lies in the explosion of the events which are going to happen in this Application. Consider this application used by large automobile manufacturing companies. They would have vehicle count in million's. Lets assume each of these vehicle will contains on average 100 parts with average servicing event as 100 days.<br /><br />So amount of service event which are to be calculated for next 5 years would be<br /><br />Assuming no of vehicle as 10 million,<br /><br />1000000 * 100 * (5*365/100) = 1825000000<br /><br />Which are approx 1825 millions records in an R.D.B.M's table. which are too much for a single R.D.B.M's table. Also with each vehicle sold by company 1825 more record will be added to the table.<br /><br />This can easily become bottleneck for traditional application [backed by R.D.B.M.S]. This will lead to various optimization e.g Horizontal Partitioning to be added to the solution. This leads to change in the application and also extra effort.&nbsp; <br /><h4 style="text-align: left;">How Hadoop helps here ?</h4>Solution which will be developed on Hadoop will be scalable by default. Once we have developed system, it will be scalable by default. If company is adding more and more vehicle, all they need to do is to add more machine in Hadoop Cluster. There is no need to do any changes in the application to deal with data explosion. <br /><br /><br /><h4 style="text-align: left;">Solution:&nbsp;</h4>I will be publishing the code on git and some explanation in next blog.<br /><br /><br /><br /></div>